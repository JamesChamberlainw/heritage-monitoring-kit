{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa7315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import fiona\n",
    "import geemap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Gee and EE \n",
    "import ee\n",
    "\n",
    "# Clustering (best for testing** due to speed)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# tif file creation\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "# Plotting and Vis \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6937ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise\n",
    "ee.Authenticate()\n",
    "ee.Initialize(project=\"jameswilliamchamberlain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c51023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basemap \n",
    "basemap_url = 'https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}'\n",
    "\n",
    "# centeral point of Samarra \n",
    "df_sites = pd.DataFrame({\n",
    "    \"longitude\": [43.823543],\n",
    "    \"latitude\": [34.340989],\n",
    "    \"name\": [\"Samarra Archaeological City\"],\n",
    "    \"category\": [\"Cultural\"],\n",
    "    \"date inscribed\": [\"2007\"],\n",
    "    \"region\": [\"Arab States\"],\n",
    "    \"url\": [\"https://whc.unesco.org/en/list/276\"],\n",
    "    \"iso\": [[\"IQ\"]]\n",
    "})\n",
    "\n",
    "# Chunks of Samarra Archaeological City \n",
    "with fiona.open(\"chunks_new.shp\") as src:\n",
    "    chunks = gpd.GeoDataFrame.from_features(src, crs=src.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65ec00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8732864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEMP REMOVE ADDITIONAL STUFF FOR QUICKER TESTING \n",
    "# tile_43641125_34108721_43915744_34336837 only \n",
    "\n",
    "chunks = chunks[chunks['file_name'] == 'tile_43641125_34108721_43915744_34336837']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = geemap.Map()\n",
    "\n",
    "if df_sites.empty:\n",
    "    print(\"No sites found for the specified URL.\")\n",
    "else:\n",
    "    m.add_points_from_xy(df_sites, x=\"longitude\", y=\"latitude\", layer_name=\"Sites\")\n",
    "    center_points = df_sites[['longitude', 'latitude']].mean().values\n",
    "    m.setCenter(center_points[0], center_points[1], 10)\n",
    "\n",
    "m.add_basemap(basemap_url, name=\"Google Satellite\", attribution=\"Google\")\n",
    "\n",
    "# add chunks from aoi \n",
    "m.add_gdf(chunks, layer_name=\"AOI\", style={\"color\": \"red\", \"fillColor\": \"red\", \"fillOpacity\": 0.1})\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13096b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subregions(chunks, sift_percentage_lon=0.5, sift_percentage_lat=0.415):\n",
    "    \"\"\"\n",
    "        Shifts all polygons to 8 positions based on half the length in longitude and latitude, to create subregions - one for each direction from the center.\n",
    "\n",
    "        Assumes aoi contains all similar polygons, and are similar to a square.\n",
    "\n",
    "        Parameters:\n",
    "            aoi (GeoDataFrame): The area of interest containing geometries.\n",
    "            sift_percentage (float): Percentage of the length to shift the center point. Default is 0.5 (50%) for half the length.\n",
    "    \"\"\"\n",
    "\n",
    "    if chunks.empty:\n",
    "        return {}\n",
    "    \n",
    "    first_polygon = chunks.geometry.iloc[0]\n",
    "\n",
    "    # take top two points of the polygon and get the length between them\n",
    "    top_points = first_polygon.exterior.coords[:2]\n",
    "    length_lon = abs(top_points[0][0] - top_points[1][0])\n",
    "\n",
    "    # calculate the shift amount\n",
    "    shift_amount_lon = length_lon * sift_percentage_lon \n",
    "    shift_amount_lat = length_lon * sift_percentage_lat\n",
    "\n",
    "    shift_directions = {\n",
    "        \"left\": (-shift_amount_lon, 0),\n",
    "        \"right\": (shift_amount_lon, 0),\n",
    "        \"up\": (0, shift_amount_lat),\n",
    "        \"down\": (0, -shift_amount_lat),\n",
    "        \"top_left\": (-shift_amount_lon, shift_amount_lat),\n",
    "        \"top_right\": (shift_amount_lon, shift_amount_lat),\n",
    "        \"bottom_left\": (-shift_amount_lon, -shift_amount_lat),\n",
    "        \"bottom_right\": (shift_amount_lon, -shift_amount_lat),\n",
    "    }\n",
    "\n",
    "    subregions = []\n",
    "        \n",
    "    # Create subregions by shifting the geometries in all directions\n",
    "    for _, (dx, dy) in shift_directions.items():\n",
    "        gdf_shifted = chunks.copy()\n",
    "        gdf_shifted[\"geometry\"] = gdf_shifted[\"geometry\"].translate(dx, dy)\n",
    "        subregions.append(gdf_shifted)\n",
    "\n",
    "    # Combine all into one GeoDataFrame\n",
    "    subregions = pd.concat(subregions, ignore_index=True)\n",
    "    return gpd.GeoDataFrame(subregions, crs=chunks.crs)\n",
    "\n",
    "def clip(chunks, aoi):\n",
    "    \"\"\"\n",
    "        Clips the chunks or subregions to the area of interest (aoi).\n",
    "\n",
    "        Parameters:\n",
    "            chunks (GeoDataFrame):      The chunks or subregions to be clipped.\n",
    "            aoi (GeoDataFrame):         The area of interest (aoi) to clip the chunks against.\n",
    "    \"\"\"\n",
    "\n",
    "    # clip to aoi \n",
    "    if chunks.empty or aoi.empty:\n",
    "        return gpd.GeoDataFrame(columns=chunks.columns.tolist(), crs=chunks.crs)\n",
    "    clipped = gpd.clip(chunks, aoi)\n",
    "    clipped = clipped[clipped.geometry.notnull()]  # Remove any null geometries\n",
    "\n",
    "    return clipped.reset_index(drop=True)\n",
    "\n",
    "subregions = create_subregions(chunks)\n",
    "subregions = clip(subregions, aoi=chunks.dissolve())\n",
    "\n",
    "# plot as one \n",
    "m.add_gdf(subregions, layer_name=\"Subregions\", style={\"color\": \"blue\", \"fillColor\": \"blue\", \"fillOpacity\": 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721056f",
   "metadata": {},
   "source": [
    "# Reference Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2ba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_points_from_geemap(map_obj, label):\n",
    "    \"\"\"\n",
    "    Collect all drawn point features from a geemap.Map that uses ee.Feature objects,\n",
    "    and return them as a GeoDataFrame with a label.\n",
    "\n",
    "    Parameters:\n",
    "        map_obj (geemap.Map): The interactive map.\n",
    "        label (str): Label to assign to all collected points.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: With geometry and 'label' columns.\n",
    "    \"\"\"\n",
    "\n",
    "    features = map_obj.draw_features\n",
    "\n",
    "    if not features:\n",
    "        return gpd.GeoDataFrame(columns=[\"geometry\", \"label\"], geometry=\"geometry\")\n",
    "\n",
    "    points = []\n",
    "    for f in features:\n",
    "        try:\n",
    "            geom = f.geometry()  # call the method\n",
    "            if geom.getInfo()[\"type\"] == \"Point\":\n",
    "                coords = geom.coordinates().getInfo()  # [lon, lat]\n",
    "                points.append(Point(coords))\n",
    "        except Exception as e:\n",
    "            print(\"Skipping feature due to error:\", e)\n",
    "\n",
    "    if not points:\n",
    "        return gpd.GeoDataFrame(columns=[\"geometry\", \"label\"], geometry=\"geometry\")\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(geometry=points)\n",
    "    gdf[\"label\"] = label\n",
    "    gdf.set_crs(\"EPSG:4326\", inplace=True)\n",
    "    return gdf\n",
    "\n",
    "def gen_basemap(basemap_url=None, aoi=gpd.GeoDataFrame(), polygons=None):\n",
    "    \"\"\"\n",
    "        Generates a basemap with the specified URL and adds polygons if provided.\n",
    "\n",
    "        Parameters:\n",
    "            basemap_url (str): The URL of the basemap to be used.\n",
    "            polygons (GeoDataFrame, optional): Polygons to be added to the map. Defaults to None.\n",
    "            aoi (GeoDataFrame): The area of interest to be displayed on the map. (can be a set of polygons or a single polygon)\n",
    "        \n",
    "        Returns:\n",
    "            geemap.Map: A geemap map object with the specified basemap and polygons.\n",
    "    \"\"\"\n",
    "    m = geemap.Map()\n",
    "\n",
    "    if not basemap_url:\n",
    "        basemap_url = 'https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}'\n",
    "        \n",
    "    m.add_basemap(basemap_url, name=\"Google Satellite\", attribution=\"Google\")\n",
    "\n",
    "    # center on aoi\n",
    "    if not aoi.empty:\n",
    "        m.add_gdf(aoi, layer_name=\"AOI\", style={\"color\": \"red\", \"fillColor\": \"red\", \"fillOpacity\": 0.1})\n",
    "        center_points = aoi.geometry.unary_union.centroid.coords[0]\n",
    "        m.setCenter(center_points[0], center_points[1], 10)\n",
    "    \n",
    "    if polygons is not None:\n",
    "        m.add_gdf(polygons, layer_name=\"Polygons\", style={\"color\": \"red\", \"fillColor\": \"red\", \"fillOpacity\": 0.1})\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879236a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of 3 maps for urban, agricultrual, water and waste\n",
    "urban_map = gen_basemap(basemap_url, aoi=chunks)\n",
    "agricultural_map = gen_basemap(basemap_url, aoi=chunks)\n",
    "water_map = gen_basemap(basemap_url, aoi=chunks)\n",
    "wasteland_map = gen_basemap(basemap_url, aoi=chunks)\n",
    "# wasteland_map = geemap.Map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb6019",
   "metadata": {},
   "outputs": [],
   "source": [
    "agricultural_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa3497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# water_map = water_waste_map\n",
    "water_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc6e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wasteland_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e410f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_map = gen_basemap(basemap_url, aoi=chunks)\n",
    "additional_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72279f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_points = collect_points_from_geemap(additional_map, label=\"Urban\")\n",
    "additional_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09657f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to labelled points in GeoDataFrame\n",
    "water_points = collect_points_from_geemap(water_map, label=\"Water\")\n",
    "agricultural_points = collect_points_from_geemap(agricultural_map, label=\"Agricultural\")\n",
    "urban_points = collect_points_from_geemap(urban_map, label=\"Urban\")\n",
    "wasteland_map = collect_points_from_geemap(wasteland_map, label=\"Wasteland\")\n",
    "\n",
    "# merge into one GeoDataFrame\n",
    "points_list = [water_points, agricultural_points, urban_points, wasteland_map]\n",
    "labels = pd.concat(points_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e15ae6",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Here we create a Sparse matrix\n",
    "\n",
    "then with the sparse matrix compare and label based on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb927e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = \"data/\"\n",
    "\n",
    "# collect paths for all csv files in the folder\n",
    "paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(pth):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            paths.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(paths)} CSV files.\")\n",
    "\n",
    "# select 2024 \n",
    "path = [p for p in paths if \"2019\" in p]\n",
    "path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a6bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(dir):\n",
    "    \"\"\"Creates two dataframes from a CSV file, one ready for clustering with additional columns removed and the other with all columns left intact.\"\"\"\n",
    "    df1 = pd.read_csv(dir)\n",
    "    df1 = df1.dropna()\n",
    "    df2_clear = df1.copy()\n",
    "    df2_clear = df2_clear.drop(columns=[\"system:index\", \".geo\"])\n",
    "    df2_clear = df2_clear.set_index(\"file_name\")\n",
    "    df1 = df1.set_index(\"file_name\")\n",
    "    df2_clear = df2_clear.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    return df1, df2_clear\n",
    "\n",
    "df1, df2_clear = prep_data(path[0])\n",
    "\n",
    "mapping = {\n",
    "            None: 0,\n",
    "            np.NaN: 0,\n",
    "            \"NaN\": 0,\n",
    "            \"None\": 0,\n",
    "            \"Water\": 1,\n",
    "            \"Agricultural\": 2,\n",
    "            \"Urban\": 3,\n",
    "            \"Wasteland\": 4,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a9636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cluster as cl \n",
    "from cluster import kmeans_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf5374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = cl.Cluster(chunks, df1, points=labels, mapping=mapping, passes=6)\n",
    "cluster.reload_state(filname_prefix=\"temp_test/test3\", filename_postfix=\"_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416caf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "from ipyleaflet import CircleMarker\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e5a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['Urban', 'Agricultural', 'Water', 'Wasteland', 'Wetland', 'Other']\n",
    "date_range = [i for i in range(2019, 2025)] \n",
    "\n",
    "collection_name = \"COPERNICUS/S2_SR_HARMONIZED\"\n",
    "\n",
    "with fiona.open(\"aoi.geojson\") as src:\n",
    "    aoi = gpd.GeoDataFrame.from_features(src, crs=src.crs)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd160c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty label list \n",
    "labels = gpd.GeoDataFrame(columns=[\"label\", \"geometry\", \"start_year\", \"end_year\"], geometry=\"geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e247923",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = geemap.Map()\n",
    "\n",
    "def map_polygon(polygon, collection_name, layer_name, yyyymmdd1=\"2024-01-01\", yyyymmdd2=\"2024-12-29\", num_tasks=10):\n",
    "\n",
    "    # Load in Year Data to Aid in Label Creation\n",
    "    collection = ee.ImageCollection(collection_name) \\\n",
    "        .filterDate(yyyymmdd1, yyyymmdd2) \\\n",
    "        .filterBounds(polygon) \\\n",
    "        .filter(ee.Filter.lt(\"CLOUDY_PIXEL_PERCENTAGE\", 20)) \\\n",
    "        .median() \\\n",
    "        .clip(polygon)\n",
    "    \n",
    "    vis = {'min': 0, 'max': 3000, 'bands': ['B4', 'B3', 'B2']}\n",
    "\n",
    "    m.addLayer(collection, vis, layer_name)\n",
    "\n",
    "    # center map on the polygon\n",
    "    coords = polygon.geometry().centroid().coordinates().getInfo()\n",
    "    m.setCenter(coords[0], coords[1], 10)\n",
    "\n",
    "label_dropdown = widgets.Dropdown(\n",
    "    options=label_list,\n",
    "    description='Label:',\n",
    "    value=label_list[0],\n",
    ")\n",
    "\n",
    "# Plots Each Year\n",
    "for year in date_range:\n",
    "    map_polygon(geemap.geopandas_to_ee(aoi), collection_name, f\"Year {year}\", yyyymmdd1=f\"{year}-01-01\", yyyymmdd2=f\"{year}-12-29\")\n",
    "\n",
    "date_slider = widgets.SelectionSlider(\n",
    "    options=[str(year) for year in range(2019, 2025)], # 2019 to 2024 for full years\n",
    "    value='2019',\n",
    "    description='Date:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True\n",
    ")\n",
    "\n",
    "time_frame_slider = widgets.SelectionSlider(\n",
    "    options=[str(month) for month in range(1, 13)], # default 12 as its the full year \n",
    "    value='3',\n",
    "    description='timeframe:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True\n",
    ")\n",
    "\n",
    "colour_dict = {\n",
    "    \"Urban\": \"blue\",\n",
    "    \"Agricultural\": \"green\",\n",
    "    \"Water\": \"cyan\",\n",
    "    \"Wasteland\": \"brown\",\n",
    "    \"Wetland\": \"lightblue\", \n",
    "} # other is grey by default\n",
    "\n",
    "def append_label_to_points(b):\n",
    "    \"\"\"Appends the selected label to the points in the labels GeoDataFrame.\"\"\"\n",
    "\n",
    "    global labels\n",
    "\n",
    "    selected_label = label_dropdown.value\n",
    "    start_year = int(date_slider.value)\n",
    "    span = int(time_frame_slider.value)\n",
    "    end_year = start_year + span - 1\n",
    "\n",
    "    # Extract drawn features (assumed to be Points)\n",
    "    all_drawn_features = m.draw_features\n",
    "    if not all_drawn_features:\n",
    "        print(\"No features drawn.\")\n",
    "        return\n",
    "    \n",
    "    points = []\n",
    "    for feature in all_drawn_features:\n",
    "        try:\n",
    "            coords = feature.geometry().coordinates().getInfo()  # [lon, lat]\n",
    "            point = Point(coords)\n",
    "            points.append(point)\n",
    "        except Exception as e:\n",
    "            print(\"Skipping feature due to error:\", e)\n",
    "\n",
    "    # create dataframe from points\n",
    "    new_labels = gpd.GeoDataFrame({\n",
    "        'label': [selected_label] * len(points),\n",
    "        'start_year': [start_year] * len(points),\n",
    "        'end_year': [end_year] * len(points),\n",
    "        'geometry': points\n",
    "    }, crs=\"EPSG:4326\")\n",
    "\n",
    "    # Clear All Drawn Features\n",
    "    m.draw_control.clear()\n",
    "\n",
    "\n",
    "    # Append to the existing labels GeoDataFrame\n",
    "    labels = pd.concat([labels, new_labels], ignore_index=True)\n",
    "    \n",
    "\n",
    "# Label Range  \n",
    "label_output = widgets.Label()\n",
    "def update_label(*args):\n",
    "    start_year = int(date_slider.value)\n",
    "    span = int(time_frame_slider.value)\n",
    "    end_year = start_year + span - 1\n",
    "    label_output.value = f\"Selected range: {start_year} to {end_year}\"\n",
    "\n",
    "date_slider.observe(update_label, names='value')\n",
    "time_frame_slider.observe(update_label, names='value')\n",
    "\n",
    "update_label() \n",
    "\n",
    "# render_btn = widgets.Button(description=\"Draw Selected Date Layer\", position=\"bottomright\") \n",
    "# widget_draw = widgets.VBox([render_btn])\n",
    "# m.add_widget(widget_draw, position=\"bottomright\")\n",
    "\n",
    "append_btn = widgets.Button(description=\"Append Label to Points\")\n",
    "append_btn.on_click(append_label_to_points)\n",
    "\n",
    "# display(widgets.HBox([date_slider, month_slider, time_frame_slider, render_btn]))\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([date_slider, time_frame_slider]),\n",
    "    widgets.HBox([label_dropdown, label_output, append_btn]),\n",
    "]))\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc63d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f3019",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pd.DataFrame({\n",
    "    \"dir\": [f\"data/{year}.csv\" for year in range(2019, 2025)],\n",
    "    \"year\": [year for year in range(2019, 2025)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ts_cluster():\n",
    "\n",
    "    cluster_list = []\n",
    "\n",
    "    yearly_labels_dict = []\n",
    "    common_labels = pd.DataFrame(columns=[\"year_start\", \"year_end\", \"label\", \"geometry\"])\n",
    "    \n",
    "    data_dir = pd.DataFrame(columns=[\"dir\", \"year\"])\n",
    "\n",
    "    start_year = 2019\n",
    "    end_year = 2024\n",
    "\n",
    "    def __init__(self, ts_point_labels, data_dir, mapping, start_year, end_year, passes=6, full_data=True):\n",
    "        \"\"\"\n",
    "            Initializes the ts_cluster object with time-series point labels, data directory, mapping, and number of passes.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.start_year = int(start_year)\n",
    "        self.end_year = int(end_year)\n",
    "        self.full_data = full_data\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        if start_year > end_year:\n",
    "            raise ValueError(\"start_year must be less than or equal to end_year\")\n",
    "        \n",
    "        common_labels, uncommon_labels = self.__reduce_to_common__(ts_point_labels)\n",
    "        self.common_labels = common_labels\n",
    "        self.yearly_labels_dict = {\n",
    "            year: self.__split_year__(year, uncommon_labels)\n",
    "            for year in range(start_year, end_year + 1)\n",
    "        }\n",
    "\n",
    "        # runtime vars \n",
    "        self.mapping = mapping\n",
    "        self.passes = passes\n",
    "\n",
    "    def __reduce_to_year__(year, label_df):\n",
    "        \"\"\"\n",
    "            Returns a DataFrame with only entries that overlap with the specified year.\n",
    "\n",
    "        Args:\n",
    "            year (int):                The year to filter the labels DataFrame.\n",
    "            label_df (GeoDataFrame):   The labels DataFrame to filter.\n",
    "\n",
    "        Returns:\n",
    "            GeoDataFrame:              A DataFrame containing only the labels that overlap with the specified year.\n",
    "        \"\"\"\n",
    "\n",
    "        year_df = label_df[(label_df['start_year'] <= year) & (label_df['end_year'] >= year)]\n",
    "        return year_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    def __reduce_to_common__(self, labels_df):\n",
    "        \"\"\"\n",
    "        Splits the labels DataFrame into two parts:\n",
    "        1. Common labels that fall within the specified start and end years.\n",
    "        2. Uncommon labels that do not fall within the specified range, and may be year specific.\n",
    "        \n",
    "        Args:\n",
    "            labels_df (GeoDataFrame):   The labels DataFrame to reduce.\n",
    "            start_year (int):           The starting year for the reduction.\n",
    "            end_year (int):             The ending year for the reduction.\n",
    "        \n",
    "        Returns:\n",
    "            GeoDataFrame:               The Reduced labels DataFrame.\n",
    "        \"\"\"\n",
    "        common_df = labels_df[(labels_df['start_year'] <= self.start_year) & (labels_df['end_year'] >= self.end_year)]\n",
    "        uncommon_df = labels_df[(labels_df['start_year'] > self.start_year) | (labels_df['end_year'] < self.end_year)]\n",
    "        return common_df.reset_index(drop=True), uncommon_df.reset_index(drop=True)  \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def __split_year__(year, label_df):\n",
    "        \"\"\"\n",
    "            Returns a DataFrame of where any label that overlaps a given year is returned.\n",
    "        \"\"\"\n",
    "\n",
    "        year_df = label_df[(label_df['start_year'] <= year) & (label_df['end_year'] >= year)]\n",
    "        # year_df['year'] = year\n",
    "        return year_df.reset_index(drop=True)\n",
    "\n",
    "    def __build_all_geometries__(self):\n",
    "        \"\"\"\n",
    "            Builds a all Geometries DataFrame from the data labels\n",
    "        \"\"\"\n",
    "        uniques = pd.DataFrame(columns=['file_name', '.geo'])\n",
    "        for i in self.data_dir.index:\n",
    "            df_dir = self.data_dir.loc[i, 'dir']\n",
    "            # print(f\"Loading Geometries {df_dir}...\")\n",
    "\n",
    "            df = pd.read_csv(df_dir)\n",
    "            df = df[['file_name', '.geo']]\n",
    "            print(f\"Loaded {len(df)} geometries from {df_dir}.\")\n",
    "\n",
    "            df = df.dropna(subset=['file_name', '.geo'])\n",
    "            df = df.reset_index(drop=True)\n",
    "            \n",
    "            uniques = pd.concat([uniques, df], ignore_index=True)\n",
    "            uniques = uniques.drop_duplicates(subset=['file_name'])\n",
    "\n",
    "            # no need to check all if we know the data IS FULL!\n",
    "            if self.full_data:\n",
    "                return uniques\n",
    "            \n",
    "        return uniques\n",
    "\n",
    "    def __build_common_labels__(self):\n",
    "        \"\"\"\n",
    "            Builds a common cluster from the common labels DataFrame. That ALL years share to avoid redundant processing.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        uniuqes_gdf = self.__build_all_geometries__()\n",
    "        uniuqes_gdf['geometry']  = uniuqes_gdf['.geo'].apply(lambda x: shape(json.loads(x)))\n",
    "        uniuqes_gdf = gpd.GeoDataFrame(uniuqes_gdf, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "        cl_cluster = cl.Cluster(subregions, None, None, 1)\n",
    "        cl_cluster.data = uniuqes_gdf.set_index(\"file_name\")\n",
    "        # cl_cluster.points = common_df.drop(columns=['start_year', 'end_year'])    \n",
    "        cl_cluster.points = self.common_labels.drop(columns=['start_year', 'end_year'])\n",
    "\n",
    "        cl_cluster.build_row_labels()\n",
    "\n",
    "        # self.base_labels = cl_cluster.labels['file_name', 'label']\n",
    "        self.base_labels = cl_cluster.labels\n",
    "        return self.base_labels\n",
    "\n",
    "    def instansiate_clusters(self, cluster_class, subregions, aoi=None, index_column=\"file_name\"):\n",
    "        \"\"\"\n",
    "            Instantiates clusters for each year in the data directory.\n",
    "        \"\"\"\n",
    "        \n",
    "        base_labelles = self.__build_common_labels__()\n",
    "        \n",
    "        cluster_list = []\n",
    "\n",
    "        for year in self.data_dir['year']:\n",
    "            data = pd.read_csv(self.data_dir.loc[data_dir['year'] == 2019, 'dir'].values[0])\n",
    "\n",
    "            # (self, subregions, df_data, mapping, passes=6, aoi=None, index_column=\"file_name\", points=gpd.GeoDataFrame()):\n",
    "            cl_cluster = cluster_class(subregions=subregions, \n",
    "                                          df_data=data, \n",
    "                                          mapping=self.mapping, \n",
    "                                          points=self.common_labels,\n",
    "                                          aoi=aoi,\n",
    "                                          index_column=index_column,\n",
    "                                          passes=self.passes)\n",
    "            \n",
    "            df_copy = base_labelles.copy()\n",
    "            df_copy = df_copy.reset_index(drop=False)\n",
    "            df_copy = df_copy[['file_name', 'label']]\n",
    "            cl_cluster.load_labels_df(df=df_copy)\n",
    "            \n",
    "            # cl_cluster.build_row_labels(additional_labels=self.yearly_labels_dict[year], update=True)\n",
    "\n",
    "            cluster_list.append(cl_cluster)\n",
    "\n",
    "        self.cluster_list = cluster_list\n",
    "\n",
    "    def fit(self, save_state=\"\"):\n",
    "        \"\"\"\n",
    "            Fits all clusters in the cluster list.\n",
    "\n",
    "            Args:\n",
    "                save_state (str):         The prefix for the filename to save the state of the clusters. If empty, no state is saved.\n",
    "        \"\"\"\n",
    "\n",
    "        for i, cl_cluster in enumerate(self.cluster_list):\n",
    "            print(f\"Fitted cluster for year {self.start_year + i} with {len(cl_cluster.labels)} labels.\")\n",
    "\n",
    "            cl_cluster.fit()\n",
    "\n",
    "            if save_state is not \"\":\n",
    "                cl_cluster.save_state(filname_prefix=save_state, filename_postfix=f\"{self.start_year + i}_state\")\n",
    "        \n",
    "        print(f\"Fitted {len(self.cluster_list)} clusters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ts = ts_cluster(ts_point_labels=labels, data_dir=data_dir, mapping=mapping, start_year=2019, end_year=2024, passes=6)\n",
    "\n",
    "cluster_ts.instansiate_clusters(cluster_class=cl.Cluster, subregions=subregions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc980329",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ts.base_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d0b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ts.cluster_list[0].data.dropna().isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for NaN values in labels\n",
    "f = cluster_ts.cluster_list[0].data\n",
    "\n",
    "filenames_with_nan = f.columns[f.isna().any()].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67fbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = cluster_ts.cluster_list[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045dc820",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426517ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(df, k):\n",
    "    # Drop non-feature columns\n",
    "    features_df = df.drop(columns=['file_name', '.geo', 'geometry', 'system:index'], errors='ignore')\n",
    "\n",
    "    # Sanity check\n",
    "    if features_df.isnull().any().any():\n",
    "        raise ValueError(\"NaNs detected in input data:\\n\" + str(features_df.isnull().sum()))\n",
    "\n",
    "    # Make sure only numeric types\n",
    "    features_df = features_df.select_dtypes(include='number')\n",
    "\n",
    "    # Fit\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(features_df)\n",
    "\n",
    "    return kmeans.labels_, df.index.tolist()\n",
    "\n",
    "kmeans_clustering(cluster_ts.cluster_list[0].data.dropna(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa642f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ts.fit(save_state=\"temp_test_fit/test3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c724371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data_dir 2019\n",
    "data_dir.loc[data_dir['year'] == 2019, 'dir'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddc08b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ts.common_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ts.cluster_list[0].labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80f008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = cl.Cluster(subregions=subregions, df_data=pd.read_csv(data_dir.loc[data_dir['year'] == 2019, 'dir'].values[0]), mapping=mapping, points=cluster_ts.common_labels, passes=6)\n",
    "cluster.reload_state(filname_prefix=\"temp_test/test3\", filename_postfix=\"_state\")\n",
    "\n",
    "cluster.labels.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
